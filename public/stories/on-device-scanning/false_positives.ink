=== False_Positives ===

= Innocent_Content
AI systems produce errors. At scale, even high accuracy rates generate large numbers of false positives. # diagram: false_positive_scale.jpg

These could include:
• Medical images sent to physicians
• Classical art or photography
• Legal intimate images between adults
• Screenshots matching flagged patterns

*   [Is this already happening?]
    -> Real_World_Examples

*   [What happens to people who get falsely flagged?]
    -> The_Chilling_Effect

= Real_World_Examples
This is already happening. # diagram: google_photos_incident.jpg

A father took photos of his toddler's groin to send to a doctor for telehealth. Google Photos' CSAM detection flagged it.

Result:
• His account was disabled
• Lost emails, photos, contacts—everything
• Police investigated
• Even after being cleared, Google refused to restore his account

His only crime? Following his doctor's instructions.

*   [How does this affect people's behavior?]
    -> The_Chilling_Effect

*   [What about due process?]
    -> No_Due_Process

= The_Chilling_Effect
When people know they're being watched, they change their behavior. # diagram: chilling_effect.jpg

You might avoid discussing sensitive medical issues, sharing art that could be misinterpreted, or sending health information to your teenager.

You don't have to be doing anything wrong—just imagining the consequences of false accusation changes what you send.

*   [How does someone defend themselves?]
    -> No_Due_Process

*   [Does this expand beyond CSAM?]
    -> Scope_Creep.Starts_With_CSAM

*   [I've seen enough. What can I do?]
    -> Conclusion.Take_Action

= No_Due_Process
The scanning happens before you even send the message. # diagram: no_warrant.jpg

No warrant. No judge reviewing probable cause. No chance to challenge the accusation.

Traditional law enforcement needs a warrant to search your phone—they have to convince a judge there's good reason. But on-device scanning searches *everyone's* phone all the time, just in case.

It's like police searching every car at every intersection to make sure nobody's transporting anything illegal.

*   [How does this expand beyond CSAM?]
    -> Scope_Creep.Starts_With_CSAM

*   [What about the nudity filter requirement?]
    -> Nudity_Filter.Age_Verification_Requirement
    
*   [I've seen enough. What can I do?]
    -> Conclusion.Take_Action

