=== Better_Solutions ===
Let's talk about what actually works. # diagram: parental_controls.png

On-device parental controls already exist on phones, tablets, and computers.

Parents can set them up to:
• Require approval before installing new apps
• Filter web content with the ability to override specific sites
• Set time limits for the device or specific apps
• Lock the device during school hours or at night
• Block or limit purchases
• Receive reports on device usage

These controls work at the device level. No central database. No government surveillance. No privacy risks for millions of people.

*   [But that doesn't stop kids from accessing social media.]
    That's actually a feature, not a bug.
    
    Different families have different needs and values.
    
    Some parents want strict filtering until age 18.
    Some prefer gradual exposure with conversation and guidance.
    Some focus on teaching critical thinking rather than blocking.
    
    Parental controls let families make these decisions themselves.
    
    -> Feature_Gating

*   [What if parents don't use these tools?]
    That's where education comes in.
    -> Parent_Education

*   [What about scanning content on the device itself?]
    That's a different approach - but it comes with serious concerns.
    -> On_Device_Scanning_Crosslink

= On_Device_Scanning_Crosslink
Some have proposed automatically scanning messages and images on phones before they're sent. # diagram: alternative_scanning.png

The idea is to detect and filter "harmful content" automatically - protecting children without needing age verification at websites.

But this approach:
• Breaks end-to-end encryption
• Creates mass surveillance infrastructure  
• Has high false positive rates
• Can be expanded to scan for anything governments want

It's worth understanding these concerns.

Would you like to explore on-device scanning?

*   [Yes, tell me about on-device scanning.]
    ~ navigateTo("on-device-scanning")
    -> END

*   [No, stick with parental controls.]
    -> Better_Solutions

=== Feature_Gating ===
Here's an even better idea that tech companies could implement: # diagram: feature_gating.png

App-specific feature controls for parents:

For WhatsApp:
• Limit messaging to approved contacts only
• Get copied on group chats
• Require approval to join new groups

For Instagram/TikTok:
• Disable "the algorithm" (only show posts from followed accounts in chronological order)
• Limit who can message or comment
• Set viewing time limits
• Require approval for new follows

For YouTube:
• Restrict to approved channels only
• Disable autoplay and recommendations  
• Enable restricted mode by default

This is more acceptable to tech firms than outright bans or age verification. It's also more effective.

The child can still access the platform and stay connected with friends. But the most harmful features—the addictive algorithms, unrestricted stranger contact, endless scrolling—are controlled by parents.

As the child demonstrates maturity, parents can gradually enable more features.

*   [Why don't we have this already?]
    Some platforms have started implementing versions of this.
    
    But there's no regulation requiring it, and companies prioritize engagement over safety.
    
    Requiring robust parental feature controls would be more effective than age bans—and wouldn't require surveillance infrastructure.
    -> Parent_Education

*   [What about education?]
    That's the other crucial piece.
    -> Parent_Education

=== Parent_Education ===
Parental controls only work if parents know about them and use them. # diagram: parent_education.png

We need comprehensive parent education programs:

How to enable parental controls:
• Step-by-step guides for each device type
• What settings are recommended for different ages  
• How to adjust controls as children mature

How to coach children about online safety:
• Not taking or sharing explicit or inappropriate content
• Understanding the consequences if they do
• What to do if they encounter harmful content
• Why certain content is harmful and addictive
• The importance of regular phone checks for safety

Understanding the "why":
• Why algorithms are designed to be addictive
• How to recognize manipulation and misinformation  
• Why certain features are particularly harmful to developing brains

*   [That sounds like a lot of work for parents.]
    It is. But so is dealing with the consequences of unsupervised internet access.
    
    And here's the thing: these aren't one-time conversations.
    
    As children grow and technology changes, parents need ongoing support.
    
    -> Ongoing_Support

=== Ongoing_Support ===
Education can't be a one-time thing. # diagram: education_cycle.png

Parents need:

Regular newsletters about new apps, features, and risks their children might encounter.

Refresher courses on updated parental control features.

Community support—forums or groups where parents can ask questions and share strategies.

Age-appropriate guidance—what's normal for a 10-year-old versus a 15-year-old.

This creates an ongoing connection rather than a "set and forget" approach.

It also shifts the conversation from "the government banned it" to "our family has rules and we understand why."

*   [This seems like a lot of infrastructure.]
    It is. But compare it to:
    
    Building age verification systems for every website.
    Maintaining a massive database of identity documents.
    Enforcing bans across millions of websites and apps.  
    Dealing with VPNs, proxies, and circumvention.
    Managing the security risks of centralized ID databases.
    
    Parent education infrastructure is simpler, more effective, and doesn't create surveillance risks.
    
    -> Why_This_Works

=== Why_This_Works ===
On-device controls and parent education address the real goal: keeping children safe. # diagram: local_vs_central_control.png

They're more effective because:
• Controls work regardless of which website or app the child tries to use
• No circumvention via VPN (the controls are on the device itself)
• Parents can adjust rules as children demonstrate maturity  
• No need to lie about age (so safety features stay enabled)
• Children are less likely to hide usage (parents are involved, not just law enforcement)

They're more privacy-preserving:
• No central database of everyone's identity
• No surveillance infrastructure  
• No security risks from data breaches
• Decisions stay within the family

They're more flexible:
• Different families can make different choices  
• Rules can adapt as children grow
• Cultural and religious values can be respected
• Special needs and circumstances can be accommodated

*   [But parents aren't always responsible.]
    True. And age verification laws don't fix that.
    
    A parent who won't install parental controls also won't prevent their child from using a VPN.
    
    A parent who isn't involved in their child's digital life won't benefit from age bans either.
    
    The difference is: our solution doesn't create surveillance infrastructure that can be misused.
    
    It empowers the parents who DO care without endangering everyone's privacy.
    
    -> Holistic_Thinking

*   [This makes sense. What else?]
    -> Holistic_Thinking
